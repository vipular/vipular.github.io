# EE798P: Audio Representation Learning (Fall 2023)

### Instructor
Vipul Arora

<!--
## Registration Note: 
- I am planning to have around 50 UGs and rest all PGs -- from EE. 
- No limit on the number of PGs.
- For UGs:
  - First come first serve.
  - Anyone who has done no other ML course will be given preference; please write in the remarks "No other ML course".

**Units:** 3-0-0-0-9 (3 hours lecture; total 9 credits)
Course link: https://hello.iitk.ac.in/course/ee698v
## TAs:
Vishal 	- vishalku@ <br>
Sumit 	- krsumit@ <br>
Vikas 	- kvikas@ <br>
Adhiraj 	- adhiraj@ <br>
Swati 	- swatisn@ <br>
Akash 	-	aaapare@ <br>
Sagnik - sagnikm@ <br>

-->



## Course Objectives:
This course aims at introducing the students to learning representations for audio. 
Speech audio can be represented as text and metadata, containing speaker information, etc.
Music audio can be represented as note sequences, pitch contours, etc.
Environmental sounds can be represented with event labels with onset/offset times.
These are directly interpretable representations. But there are also statistically learnt latent representations that may not be directly interpretable but are useful for downstream tasks such as retrieval, analysis or extracting directly interpretable representations.
The course will have reading assignments (papers). The lectures will focus on mathematical principles, and there will be coding based assignments for implementation. 

<!--
## Registration: (Updated on 23 Nov, 10AM)

- No more space for UG students.
- All PG students will be accepted and are encouraged to apply.
- For auditing the course, please send a request to rashmiy@iitk.ac.in.
-->

## Pre-requisites:
- Basic course on machine learning
- Digital signal processing (EE301A or equivalent)
- Basics of Programming (ESc101 or equivalent)

<!-- The course will need a strong background in linear algebra and probability theory. -->

## Topics:

- Basics of Digital Signal Processing
- Basics of Audio processing
- Audio classification
  - Event-wise and segment-wise
- Audio to Text
  - HMMs 
  - CTC training
- Audio retrieval
  - Information retrieval
  - Unsupervised representations

<!-- ## Lecture Plan

| Week of 2021 | Topics |
|----:|----|
|3| Introduction|
|4| Data structures and Algorithms|
|5| Data structures and Algorithms|
|6| Neural Networks |
|7| Time Series Modeling |
|8| Time Series Modeling | 
|9| Mid-sem Exam |
|10| Vacation |
|11| Attention Models, Few shot learning, domain adaptation, explainable ML |
|12| Sampling - Monte Carlo Methods | 
|13| Sampling - Monte Carlo Methods | 
|14| Variational AutoEncoder |
|15| Generative Adversarial Network |
|16| Normalizing Flows |
|17| Projects |
|18| Projects |
|19| End-sem Exam |

<!-- 
<sup>1</sup> Supervised and Unsupervised learning, Linear Classification and Regression, Evaluation Metrics 
<sup>2</sup> Multi-class classification and Multi-label classification, different kinds of non-linearities, objective functions and learning methods 
<sup>2</sup> Hidden Markov Models, Finite State Transducers and Dynamic Programming
--> -->

<!-- ## Grading Scheme
1. Continuous Assessment – 20% <br>
Assignment - 6.7%, Quiz - 13.3%
2. Mid-semester Exam – 33.3% <br>
Written exam
3. Project – 46.7% <br>
4. Bonus: Demo 5%, Paper 5%

### Details of project:
- The project presentations will include an individual viva too to assess each individual member's contribution.
- Project presentations will be held from **13 May onwards**. We will release slots which you can fill.
- Students with difficulties, such as COVID complications, can contact the instructor for an extension in deadline (submission by 17 May, presentation on 18 May). Subject to instructor's approval.

| Details | Marks (out of 35) |
|--|--|
| Problem definition: straightforward + variant | 5 marks |
| No. of methods used (using available libraries) | 1 mark/method |
| No. of methods used (self implemented) | 5 marks/method |
| Report (up to 4 pages + references) Template: https://nips.cc/Conferences/2020/PaperInformation/StyleFiles | 10 marks |
| Codes (readable + reproducible) | 10 marks |

### Plagiarism Penalty:<br>
As heavy as possible. Zero-tolerance policy.

## Projects
- Group of 2-3 students
- We will provide the datasets - students can choose or design problems around them
--> 

## References:
  This course will take excerpts from some standard books on machine
  learning and signal processing. But it will largely be based on
  articles and research papers in ML and audio conferences (e.g.,
  NeurIPS, ICML, ICLR, Interspeech, ICASSP, etc.) and journals (e.g., IEEE
  signal processing magazine, etc.). 

Books:

  - "Pattern Recognition and Machine Learning", C.M. Bishop, 2nd
    Edition, Springer, 2011. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf

<!--
  - https://ccrma.stanford.edu/~jos/sasp/
  - "Deep Learning", I. Goodfellow, Y, Bengio, A. Courville, MIT
    Press, 2016. 
  - https://www.youtube.com/watch?v=0ALKGR0I5MA - Basic Sound Processing in Python | SciPy 2015 | Allen Downey
  - Introduction to Audio Analysis: MATLAB approach, Theodoros Giannakopoulos and Aggelos Pikrakis
  - "Introduction to Audio Signal Processing", Warren L. G. Koontz,
    RIT Press, 2016.

  - https://opensource.com/article/19/9/audio-processing-machine-learning-python


Slots for presentation: (max. 20 mins per slot)
13 May: 10AM-2PM: slots 1-12
14 May: 10AM-2PM: slots 13-24


-->
